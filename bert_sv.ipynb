{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/dehghani/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from attention_graph_util import *\n",
    "import seaborn as sns\n",
    "import itertools \n",
    "import matplotlib as mpl\n",
    "import networkx as nx\n",
    "import os\n",
    "from util import constants\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "import pandas as pd\n",
    "\n",
    "from util.models import MODELS\n",
    "from util.tasks import TASKS\n",
    "#from dnotebook_utils import *\n",
    "from attention_graph_util import *\n",
    "%matplotlib inline\n",
    "from util.config_util import get_task_params\n",
    "from notebooks.notebook_utils import *\n",
    "from util import inflect\n",
    "\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import spearmanr\n",
    "import math\n",
    "\n",
    "\n",
    "rc={'font.size': 10, 'axes.labelsize': 10, 'legend.fontsize': 10.0, \n",
    "    'axes.titlesize': 32, 'xtick.labelsize': 20, 'ytick.labelsize': 16}\n",
    "plt.rcParams.update(**rc)\n",
    "mpl.rcParams['axes.linewidth'] = .5 #set the value globally\n",
    "\n",
    "import torch\n",
    "from transformers import *\n",
    "from transformers import BertConfig, BertForMaskedLM, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade transformers\n",
    "!pip install networkx\n",
    "!pip install --upgrade matplotlib\n",
    "!pip install --upgrade seaborn\n",
    "\n",
    "\n",
    "!pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Overwrite dataset info from restored data version.\n",
      "INFO:absl:Constructing tf.data.Dataset for split validation, from ../InDist/data/word_sv_agreement/0.1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab len:  10032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Constructing tf.data.Dataset for split test, from ../InDist/data/word_sv_agreement/0.1.0\n",
      "INFO:absl:Constructing tf.data.Dataset for split train, from ../InDist/data/word_sv_agreement/0.1.0\n"
     ]
    }
   ],
   "source": [
    "task_name = 'word_sv_agreement_lm'\n",
    "task_params = get_task_params(batch_size=1)\n",
    "task = TASKS[task_name](task_params, data_dir='../InDist/data')\n",
    "cl_token = task.sentence_encoder().encode(constants.bos)\n",
    "task_tokenizer = task.sentence_encoder()._tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c2d2392a0eb447b82262feec37ae995",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=442.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import torch\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertForMaskedLM.from_pretrained('distilbert-base-uncased',\n",
    "                                        output_hidden_states=True,\n",
    "                                        output_attentions=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def offset_convertor(encoded_input_task, task_offset, task_encoder, tokenizer):\n",
    "    string_part1 = task_encoder.decode(encoded_input_task[:task_offset])\n",
    "    tokens_part1 = tokenizer.tokenize(string_part1)\n",
    "    \n",
    "    return len(tokens_part1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "many NNS of woodland remain and support a JJ sector in the southern portion of the state .\n",
      "21 ['cls', 'many', 'n', '##ns', 'of', 'woodland', 'remain', 'and', 'support', 'a', 'jj', 'sector', 'in', 'the', 'southern', 'portion', 'of', 'the', 'state', '.', 'sep']\n",
      "torch.Size([1, 21, 30522])\n",
      "(6, 12, 21, 21)\n",
      "torch.Size([1, 21, 768])\n",
      "tensor(-3811819.5000, grad_fn=<SumBackward0>)\n",
      "torch.Size([1, 21, 768])\n"
     ]
    }
   ],
   "source": [
    "for x,y in task.test_dataset:\n",
    "    sentence = task.sentence_encoder().decode(x[0][1:])\n",
    "    print(sentence)\n",
    "    break\n",
    "\n",
    "tokens = ['cls']+tokenizer.tokenize(sentence)+['sep']\n",
    "print(len(tokens), tokens)\n",
    "tf_input_ids = tokenizer.encode(sentence)\n",
    "input_ids = torch.tensor([tf_input_ids])\n",
    "logits, all_hidden_states, all_attentions = model(input_ids)\n",
    "print(logits.shape)\n",
    "_attentions = [att.detach().numpy() for att in all_attentions]\n",
    "attentions_mat = np.asarray(_attentions)[:,0]\n",
    "print(attentions_mat.shape)\n",
    "\n",
    "embeded_inputs = torch.autograd.Variable(model.distilbert.embeddings(input_ids), requires_grad=True)\n",
    "logits, all_hidden_states, all_attentions = model(inputs_embeds=embeded_inputs)\n",
    "print(embeded_inputs.shape)\n",
    "\n",
    "\n",
    "lsum = logits.sum()\n",
    "print(lsum)\n",
    "\n",
    "lsum.backward()\n",
    "embeded_inputs.require_grad = True\n",
    "print(embeded_inputs.grad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Constructing tf.data.Dataset for split validation, from ../InDist/data/word_sv_agreement/0.1.0\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:00,  5.31it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'ready', 'to', 'serve', 'at', 'every', 'opportunity', ',', 'yet', 'making', 'sure', 'that', 'your', 'fellow', 'servers', '[MASK]', 'an', 'equal', 'chance', '.', '[SEP]']\n",
      "['[CLS]', 'reviewed', 'journals', '[MASK]', 'of', 'varying', 'degrees', 'of', 'reliability', '.', '[SEP]']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "3it [00:00,  6.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "4it [00:00,  5.60it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'operation', 'since', '1871', ',', 'the', 'network', '[MASK]', 'presently', 'about', 'long', ',', 'and', 'comprises', '10', 'lines', '.', '[SEP]']\n",
      "['[CLS]', 'peak', 'times', ',', 'the', 'n', '##np', 'route', '[MASK]', 'via', 'the', 'n', '##np', 'guided', 'n', '##np', 'to', 'cambridge', '.', '[SEP]']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "5it [00:00,  5.18it/s]\u001b[A\u001b[A\n",
      "\n",
      "8it [00:01,  6.47it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'first', 'requirement', '(', 'n', '##np', ')', 'simply', '[MASK]', 'that', 'a', 'cd', 'should', 'be', 'a', 'distribution', 'on', 'the', 'parameter', 'space', '.', '[SEP]']\n",
      "['[CLS]', 'letters', '[MASK]', 'small', 'and', 'jj', '.', '[SEP]']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "10it [00:01,  6.80it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'women', 'the', 'number', '[MASK]', 'one', 'in', 'forty', 'and', 'the', 'n', '##ns', 'are', 'more', 'likely', 'to', 'be', 'prison', 'staff', 'members', '.', '[SEP]']\n",
      "['[CLS]', 'n', '##n', 'link', '[MASK]', 'because', 'the', 'company', 'uses', 'n', '##n', 'products', '.', '[SEP]']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "12it [00:01,  7.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "13it [00:01,  7.06it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'support', 'he', '[MASK]', 'a', 'fine', 'editor', ',', 'but', 'has', 'too', 'little', 'edit', '##s', '.', '[SEP]']\n",
      "['[CLS]', 'method', '[MASK]', 'the', 'magnetic', 'n', '##n', 'that', 'the', 'n', '##n', 'experiences', ',', 'constant', 'over', 'the', 'n', '##n', \"'\", 's', 'normal', 'n', '##n', 'range', '.', '[SEP]']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [00:01,  7.00it/s]\n"
     ]
    }
   ],
   "source": [
    "all_examples_x = []\n",
    "all_examples_vp = []\n",
    "all_examples_y = []\n",
    "\n",
    "all_examples_attentions = []\n",
    "all_examples_blankout_relevance = []\n",
    "all_examples_grads = []\n",
    "all_examples_inputgrads = []\n",
    "n_batches = 10\n",
    "\n",
    "all_examples_accuracies = []\n",
    "\n",
    "infl_eng = inflect.engine()\n",
    "verb_infl, noun_infl = gen_inflect_from_vocab(infl_eng, '../InDist/notebooks/wiki.vocab')\n",
    "\n",
    "test_data = task.databuilder.as_dataset(split='validation', batch_size=1)\n",
    "for examples in tqdm(test_data):\n",
    "    sentence = task.sentence_encoder().decode(examples['sentence'][0][1:])\n",
    "    if len(examples['sentence'][0][1:]) > 20:\n",
    "        continue\n",
    "    \n",
    "    verb_position = examples['verb_position'][0].numpy()-1  #+1 because of adding cls.\n",
    "    verb_position = offset_convertor(examples['sentence'][0], verb_position, task.sentence_encoder(), tokenizer)\n",
    "    \n",
    "    sentence = tokenizer.tokenize(sentence)\n",
    "    \n",
    "    all_examples_vp.append(verb_position)\n",
    "    sentence[verb_position] = tokenizer.mask_token\n",
    "    \n",
    "    tf_input_ids = tokenizer.encode(sentence)\n",
    "    input_ids = torch.tensor([tf_input_ids])\n",
    "    sentence = tokenizer.tokenize(tokenizer.decode(tf_input_ids))\n",
    "    print(sentence)\n",
    "\n",
    "    \n",
    "    s_shape = input_ids.shape\n",
    "    batch_size, length = s_shape[0], s_shape[1]\n",
    "    actual_verb = examples['verb'][0].numpy().decode(\"utf-8\")\n",
    "    inflected_verb = verb_infl[actual_verb] \n",
    "\n",
    "\n",
    "    actual_verb_index = tokenizer.encode(tokenizer.tokenize(actual_verb))[1]\n",
    "    inflected_verb_index = tokenizer.encode(tokenizer.tokenize(inflected_verb))[1]\n",
    "\n",
    "    all_examples_x.append(input_ids)\n",
    "    embeded_inputs = torch.autograd.Variable(model.distilbert.embeddings(input_ids), requires_grad=True)\n",
    "    predictions = model(inputs_embeds=embeded_inputs)\n",
    "    logits = predictions[0][0]\n",
    "\n",
    "    \n",
    "        \n",
    "    probs = torch.nn.Softmax(dim=-1)(logits)\n",
    "    actual_verb_score = probs[verb_position][actual_verb_index]\n",
    "    inflected_verb_score = probs[verb_position][inflected_verb_index]\n",
    "    \n",
    "    main_diff_score = actual_verb_score - inflected_verb_score\n",
    "    \n",
    "    all_examples_accuracies.append(main_diff_score > 0)\n",
    "    \n",
    "    logits_sum = logits.sum()\n",
    "    actual_verb_score.backward()\n",
    "    grads = embeded_inputs.grad\n",
    "    grad_scores = abs(np.sum(grads.detach().numpy(), axis=-1))\n",
    "    input_grad_scores = abs(np.sum((grads * embeded_inputs).detach().numpy(), axis=-1))\n",
    "    all_examples_grads.append(grad_scores)\n",
    "    all_examples_inputgrads.append(input_grad_scores)\n",
    "    \n",
    "    hidden_states, attentions = predictions[-2:]\n",
    "    _attentions = [att.detach().numpy() for att in attentions]\n",
    "    attentions_mat = np.asarray(_attentions)[:,0]\n",
    "\n",
    "    all_examples_attentions.append(attentions_mat)\n",
    "    \n",
    "    n_batches -= 1\n",
    "    if n_batches <= 0:\n",
    "        break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_att_relevance(full_att_mat, input_tokens, layer=-1, output_index=0):\n",
    "    raw_rel = full_att_mat[layer].sum(axis=0)[output_index]/full_att_mat[layer].sum(axis=0)[output_index].sum()\n",
    "    \n",
    "    return raw_rel\n",
    "\n",
    "\n",
    "def get_joint_relevance(full_att_mat, input_tokens, layer=-1, output_index=0):\n",
    "    att_sum_heads =  full_att_mat.sum(axis=1) / full_att_mat.shape[1]\n",
    "    joint_attentions = compute_joint_attention(att_sum_heads, add_residual=True)\n",
    "    relevance_attentions = joint_attentions[layer][output_index]\n",
    "    return relevance_attentions\n",
    "\n",
    "\n",
    "def get_flow_relevance(full_att_mat, input_tokens, layer, output_index):\n",
    "    \n",
    "    input_tokens = input_tokens\n",
    "    res_att_mat = full_att_mat.sum(axis=1)/full_att_mat.shape[1]\n",
    "    res_att_mat = res_att_mat + np.eye(res_att_mat.shape[1])[None,...]\n",
    "    res_att_mat = res_att_mat / res_att_mat.sum(axis=-1)[...,None]\n",
    "\n",
    "    res_adj_mat, res_labels_to_index = get_adjmat(mat=res_att_mat, input_tokens=input_tokens)\n",
    "    \n",
    "    A = res_adj_mat\n",
    "    res_G=nx.from_numpy_matrix(A, create_using=nx.DiGraph())\n",
    "    for i in np.arange(A.shape[0]):\n",
    "        for j in np.arange(A.shape[1]):\n",
    "            nx.set_edge_attributes(res_G, {(i,j): A[i,j]}, 'capacity')\n",
    "\n",
    "\n",
    "    output_nodes = ['L'+str(layer+1)+'_'+str(output_index)]\n",
    "    input_nodes = []\n",
    "    for key in res_labels_to_index:\n",
    "        if res_labels_to_index[key] < full_att_mat.shape[-1]:\n",
    "            input_nodes.append(key)\n",
    "    \n",
    "    flow_values = compute_node_flow(res_G, res_labels_to_index, input_nodes=input_nodes, output_nodes=output_nodes, length=full_att_mat.shape[-1])\n",
    "    \n",
    "    n_layers = full_att_mat.shape[0]\n",
    "    length = full_att_mat.shape[-1]\n",
    "    final_layer_attention = flow_values[(layer+1)*length:, layer*length:(layer+1)*length]\n",
    "    relevance_attention_flow = final_layer_attention[output_index]\n",
    "\n",
    "    return relevance_attention_flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 12, 21, 21)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_examples_attentions[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|██████████| 10/10 [00:00<00:00, 2236.84it/s]\n",
      "\n",
      "\n",
      "100%|██████████| 10/10 [00:00<00:00, 1624.63it/s]\n",
      "\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute raw relevance scores ...\n",
      "compute joint relevance scores ...\n",
      "compute flow relevance scores ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 10%|█         | 1/10 [00:03<00:32,  3.65s/it]\u001b[A\u001b[A\n",
      "\n",
      " 20%|██        | 2/10 [00:04<00:21,  2.67s/it]\u001b[A\u001b[A\n",
      "\n",
      " 30%|███       | 3/10 [00:05<00:16,  2.42s/it]\u001b[A\u001b[A\n",
      "\n",
      " 40%|████      | 4/10 [00:08<00:15,  2.56s/it]\u001b[A\u001b[A\n",
      "\n",
      " 50%|█████     | 5/10 [00:13<00:15,  3.15s/it]\u001b[A\u001b[A\n",
      "\n",
      " 70%|███████   | 7/10 [00:17<00:08,  2.81s/it]\u001b[A\u001b[A\n",
      "\n",
      " 80%|████████  | 8/10 [00:18<00:04,  2.21s/it]\u001b[A\u001b[A\n",
      "\n",
      " 90%|█████████ | 9/10 [00:19<00:01,  1.89s/it]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 10/10 [00:27<00:00,  2.78s/it]\u001b[A\u001b[A\n"
     ]
    }
   ],
   "source": [
    "print(\"compute raw relevance scores ...\")\n",
    "all_examples_raw_relevance = {}\n",
    "for l in np.arange(5,6):\n",
    "    all_examples_raw_relevance[l] = []\n",
    "    for i in tqdm(np.arange(len(all_examples_x))):\n",
    "        tokens = tokenizer.tokenize(tokenizer.decode(all_examples_x[i][0].numpy()))\n",
    "        vp = all_examples_vp[i]\n",
    "        length = len(tokens)\n",
    "        attention_relevance = get_raw_att_relevance(all_examples_attentions[i], tokens, layer=l, output_index=vp)\n",
    "        all_examples_raw_relevance[l].append(np.asarray(attention_relevance))\n",
    "\n",
    "print(\"compute joint relevance scores ...\")\n",
    "all_examples_joint_relevance = {}\n",
    "for l in np.arange(5,6):\n",
    "    all_examples_joint_relevance[l] = []\n",
    "    for i in tqdm(np.arange(len(all_examples_x))):\n",
    "        tokens = tokenizer.tokenize(tokenizer.decode(all_examples_x[i][0].numpy()))\n",
    "        vp = all_examples_vp[i]\n",
    "        length = len(tokens)\n",
    "        attention_relevance = get_joint_relevance(all_examples_attentions[i], tokens, layer=l, output_index=vp)\n",
    "        all_examples_joint_relevance[l].append(np.asarray(attention_relevance))\n",
    "    \n",
    "print(\"compute flow relevance scores ...\")\n",
    "all_examples_flow_relevance = {}\n",
    "for l in np.arange(5,6):\n",
    "    all_examples_flow_relevance[l] = []\n",
    "    for i in tqdm(np.arange(len(all_examples_x))):\n",
    "        tokens = tokenizer.tokenize(tokenizer.decode(all_examples_x[i][0].numpy()))\n",
    "        vp = all_examples_vp[i]\n",
    "        length = len(tokens)\n",
    "        attention_relevance = get_flow_relevance(all_examples_attentions[i], tokens, layer=l, output_index=vp)\n",
    "        all_examples_flow_relevance[l].append(np.asarray(attention_relevance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###############Layer  5 #############\n",
      "raw grad\n",
      "(21,) (21,)\n",
      "0.15722765381669673 0.3068715774066908\n",
      "joint grad\n",
      "(21,) (21,)\n",
      "0.2847923704897725 0.26784481005012173\n",
      "flow grad\n",
      "(21,) (21,)\n",
      "0.278563534317075\n"
     ]
    }
   ],
   "source": [
    "for l in np.arange(5,6):\n",
    "    print(\"###############Layer \",l, \"#############\")\n",
    "\n",
    "    print('raw grad')\n",
    "    print(all_examples_raw_relevance[l][0].shape, all_examples_grads[0][0].shape)\n",
    "    raw_sps_grad = []\n",
    "    for i in np.arange(len(all_examples_x)):\n",
    "        sp = spearmanr(all_examples_raw_relevance[l][i],all_examples_grads[i][0])\n",
    "        if not math.isnan(sp[0]):\n",
    "            raw_sps_grad.append(sp[0])\n",
    "        else:\n",
    "            raw_sps_grad.append(0)\n",
    "        \n",
    "    print(np.mean(raw_sps_grad), np.std(raw_sps_grad))\n",
    "\n",
    "    \n",
    "    print('joint grad')\n",
    "    print(all_examples_joint_relevance[l][0].shape, all_examples_grads[0][0].shape)\n",
    "    joint_sps_grad = []\n",
    "    for i in np.arange(len(all_examples_x)):\n",
    "        sp = spearmanr(all_examples_joint_relevance[l][i],all_examples_grads[i][0])\n",
    "        if not math.isnan(sp[0]):\n",
    "            joint_sps_grad.append(sp[0])\n",
    "        else:\n",
    "            joint_sps_grad.append(0)\n",
    "        \n",
    "    print(np.mean(joint_sps_grad), np.std(joint_sps_grad))\n",
    "\n",
    "  \n",
    "    print('flow grad')\n",
    "    print(all_examples_joint_relevance[l][0].shape, all_examples_grads[0][0].shape)\n",
    "    flow_sps_grad = []\n",
    "    for i in np.arange(len(all_examples_x)):\n",
    "        sp = spearmanr(all_examples_flow_relevance[l][i],all_examples_grads[i][0])\n",
    "        if not math.isnan(sp[0]):\n",
    "            flow_sps_grad.append(sp[0])\n",
    "        else:\n",
    "            flow_sps_grad.append(0)\n",
    "        \n",
    "    print(np.mean(flow_sps_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_examples_joint_relevance[l][0].shape)\n",
    "print(all_examples_flow_relevance[l][0].shape)\n",
    "print(all_examples_blankout_relevance[0].numpy().shape)\n",
    "print(all_examples_inputgrads[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_examples_inputgrads[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = model\n",
    "\n",
    "sentences = []\n",
    "all_atts = []\n",
    "all_main_probs = []\n",
    "all_index_probs = []\n",
    "all_gradient_scores = []\n",
    "all_inputgradient_scores = []\n",
    "prob_fn = task.get_probs_fn()\n",
    "count = 0\n",
    "for x, y in task.test_dataset:\n",
    "    \n",
    "     #Manually add cls token:\n",
    "    batch_size = len(x)\n",
    "    cl_token = tf.reshape(tf.convert_to_tensor(cl_token[0], dtype=tf.int64)[None], (-1,1))\n",
    "    cl_tokens = tf.tile(cl_token, (batch_size, 1))\n",
    "    x = tf.concat([cl_tokens, x], axis=-1)\n",
    "    \n",
    "    # Get gradient scores \n",
    "    input_embeddings, input_shape, padding_mask, past = model.get_input_embeddings(x, training=False, add_cls=False)\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(input_embeddings)\n",
    "        outputs = model_1.call_with_embeddings(input_embeddings, input_shape, padding_mask, past)\n",
    "        logits = outputs[0]\n",
    "        probs = tf.nn.softmax(logits, axis=-1)\n",
    "        diff_probs = probs[:,0] - probs[:,1]\n",
    "        \n",
    "    grads = tape.gradient(diff_probs, input_embeddings)\n",
    "    grad_scores = tf.abs(tf.reduce_sum(grads, axis=-1))\n",
    "    input_grad_scores = tf.abs(tf.reduce_sum(tf.multiply(grads, input_embeddings), axis=-1))\n",
    "    \n",
    "    \n",
    "    all_gradient_scores.extend(grad_scores)\n",
    "    all_inputgradient_scores.extend(input_grad_scores)\n",
    "    \n",
    "    \n",
    "    max_len = x.shape[1]\n",
    "    all_outputs = model_1.detailed_call(x, training=False, add_cls=False)\n",
    "    main_logits = all_outputs[0]\n",
    "    attentions = all_outputs[6]\n",
    "    _attentions = [att.numpy() for att in attentions]\n",
    "    attentions = np.transpose(np.asarray(_attentions), (1,0,2,3,4))\n",
    "    main_probs = prob_fn(main_logits, y, 1)\n",
    "    batch_indexes = tf.range(len(y), dtype=tf.int64)\n",
    "    indexes = tf.concat([batch_indexes[:,None], y[:,None]], axis=1)\n",
    "    correct_main_probs = tf.gather_nd(main_probs, indexes).numpy()\n",
    "\n",
    "    sentences.append(task.databuilder.sentence_encoder().decode(x[0]))\n",
    "    all_atts.extend(attentions)\n",
    "    all_main_probs.extend(correct_main_probs)\n",
    "    all_index_probs.append([])\n",
    "    \n",
    "    # This loop can be optimized so that there is only one call...\n",
    "    new_xz = []\n",
    "    for i in np.arange(0,max_len):\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        unktoken = task.databuilder.sentence_encoder().encode(constants.unk)\n",
    "        unk = tf.reshape(tf.convert_to_tensor(unktoken, dtype=tf.int64)[None], (-1,1))\n",
    "        unks = tf.tile(unk, (batch_size, 1))\n",
    "        new_x = tf.concat([x[:,:i], unks, x[:,i+1:]], axis=-1)\n",
    "        new_xz.extend(new_x)\n",
    "    \n",
    "    new_x = np.asarray(new_xz)\n",
    "    logits = model_1(new_x, training=False, add_cls=False)\n",
    "    probs = prob_fn(logits, y, 1)\n",
    "    \n",
    "    batch_indexes = tf.range(len(probs), dtype=tf.int64)\n",
    "    yz = tf.tile(y, (len(probs),))\n",
    "\n",
    "    indexes = tf.concat([batch_indexes[:,None], yz[:,None]], axis=1)\n",
    "    \n",
    "    correct_probs = tf.gather_nd(probs, indexes).numpy()\n",
    "    all_index_probs[-1].extend(abs(correct_main_probs - correct_probs))\n",
    "    count += 1\n",
    "    if count > 100:\n",
    "        break\n",
    "    print (count, end=\"\\r\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
